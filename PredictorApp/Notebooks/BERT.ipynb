{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "BERT.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "caviH29w7RxD",
    "outputId": "191691f9-5aea-4f17-cf8b-3c79b83cce2b"
   },
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "torch.cuda.is_available()"
   ],
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n",
      "Tesla K80\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BEews2un7I-h"
   },
   "source": [
    "!pip install transformers\n",
    "!pip3 install emoji"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "u5vIgCmFiHWq"
   },
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import torch\n",
    "\n",
    "inputlength = 128\n",
    "\n",
    "\n",
    "class BERTTweet:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False, normalization=True)\n",
    "        self.bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\").to('cuda:0')\n",
    "\n",
    "    def tweet_to_vec_string(self, tweet):\n",
    "        vec = self.tweet_to_vec(tweet)\n",
    "        return torch.flatten(vec).tolist()\n",
    "\n",
    "    def tweet_to_vec(self, tweet):\n",
    "        tokens = self.tokenizer.encode(tweet)\n",
    "\n",
    "        if len(tokens) > inputlength:\n",
    "            tokens = tokens[0:inputlength]\n",
    "\n",
    "        input_ids = torch.tensor([tokens]).to('cuda:0')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            features = self.bertweet(input_ids)\n",
    "\n",
    "        return features.pooler_output\n"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ih8PrkQI676k"
   },
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class BERTCalculator:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tweet_bert = BERTTweet()\n",
    "\n",
    "    def pre_processing(self, tweet):\n",
    "        tweet = tweet.replace('\\n', '')\n",
    "        tweet = tweet.replace('\\t', '')\n",
    "        tweet = tweet.replace('\\r', '')\n",
    "        return tweet\n",
    "\n",
    "    def pipeline(self, index, row, data_set):\n",
    "        tweet = row[\"text\"]\n",
    "\n",
    "        tweet = self.pre_processing(tweet)\n",
    "\n",
    "        vec = self.tweet_bert.tweet_to_vec_string(tweet)\n",
    "        data_set._set_value(index, 'bert', vec)\n",
    "\n",
    "\n",
    "def vectorize_tweets(data):\n",
    "    print(\"VectorizeBert started\")\n",
    "    mechanism = BERTCalculator()\n",
    "\n",
    "    data[\"bert\"] = [[]] * len(data)\n",
    "    t0 = time.time()\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "\n",
    "        try:\n",
    "            mechanism.pipeline(index, row, data)\n",
    "\n",
    "        except IndexError as err:\n",
    "            print(err)\n",
    "\n",
    "        except Exception as err:\n",
    "            print(row[\"text\"])\n",
    "            raise err\n",
    "\n",
    "        if index % 1000 == 0:\n",
    "            print(\"index\", index, \" time: \", (time.time() - t0))\n",
    "\n",
    "    print(\"VectorizeBert done\")\n",
    "\n",
    "    return data\n"
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "badjF9t395q9",
    "outputId": "ab71637d-6c3b-44b9-ea79-8fa3531df118"
   },
   "source": [
    "root_path = \"/content/drive/MyDrive/Dev/\"\n",
    "file = \"03 2018_en\"\n",
    "try:\n",
    "    data = pd.read_csv(root_path + file + \".csv\", sep=\";\")\n",
    "except:\n",
    "    data = pd.read_csv(root_path + file + \".csv\", lineterminator=\"\\n\")\n",
    "\n",
    "\n",
    "print(\"Loaded\", file, \" \", data.shape)\n",
    "\n",
    "data = vectorize_tweets(data)\n",
    "data.to_csv(path_or_buf=root_path + file + \"_bert.csv\")\n"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded 03 2018_en   (958245, 13)\n",
      "VectorizeBert started\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bert = \"\"\n",
    "for i in range(0,768):\n",
    "    bert += \",bert\" +str(i)\n",
    "\n",
    "bert += \",\"\n",
    "\n",
    "with open(\"../Data/2018tweets/Objects/(60Min).csv\", 'r') as infile, \\\n",
    "     open('../Data/2018tweets/Objects/(60Min)bert.csv', 'w') as outfile:\n",
    "    data = infile.read()\n",
    "    data = data.replace('\"', '')\n",
    "    data = data.replace('[', '')\n",
    "    data = data.replace(']', '')\n",
    "    data = data.replace(',bert,', bert)\n",
    "\n",
    "    outfile.write(data)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}